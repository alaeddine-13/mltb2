# Machine Learning Toolbox 2 - MLTB2

[![MIT License](https://img.shields.io/github/license/telekom/mltb2)](https://github.com/telekom/mltb2/blob/main/LICENSE)
[![Python Version](https://img.shields.io/pypi/pyversions/mltb2)](https://www.python.org)
[![pypi](https://img.shields.io/pypi/v/mltb2.svg)](https://pypi.python.org/pypi/mltb2)
<br/>
[![pytest](https://github.com/telekom/mltb2/actions/workflows/pytest.yml/badge.svg)](https://github.com/telekom/mltb2/actions/workflows/pytest.yml)
[![Static Code Checks](https://github.com/telekom/mltb2/actions/workflows/static_checks.yml/badge.svg)](https://github.com/telekom/mltb2/actions/workflows/static_checks.yml)
[![GitHub issues](https://img.shields.io/github/issues-raw/telekom/mltb2)](https://github.com/telekom/mltb2/issues)

ðŸ“¦ A box of machine learning tools. ðŸ“¦

## Main Components

[`from mltb2.somajo import SoMaJoSentenceSplitter`](https://github.com/telekom/mltb2/blob/main/mltb2/somajo.py)\
Split texts into sentences. For German and English language.
This is done with the [SoMaJo](https://github.com/tsproisl/SoMaJo) tool.

[`from mltb2.transformers import TransformersTokenCounter`](https://github.com/telekom/mltb2/blob/main/mltb2/transformers.py)\
Count tokens made by a [Transformers](https://github.com/huggingface/transformers) tokenizer.

[`from mltb2.somajo_transformers import TextSplitter`](https://github.com/telekom/mltb2/blob/main/mltb2/somajo_transformers.py)\
Split the text into sections with a specified maximum token length.
Does not divide words, but always whole sentences.

[`from mltb2.optuna import SignificanceRepeatedTrainingPruner`](https://github.com/telekom/mltb2/blob/main/mltb2/optuna.py)\
An [Optuna pruner](https://optuna.readthedocs.io/en/stable/reference/pruners.html)
to use statistical significance (a t-test which serves as a heuristic) to stop
unpromising trials early, avoiding unnecessary repeated training during cross validation.

## Installation

MLTB2 is available at [the Python Package Index (PyPI)](https://pypi.org/project/mltb2/).
It can be installed with pip:

```bash
pip install mltb2
```

Some optional dependencies might be necessary. You can install all of them with:

```bash
pip install mltb2[optional]
```

## SignificanceRepeatedTrainingPruner Doc

This is an [Optuna pruner](https://optuna.readthedocs.io/en/stable/reference/pruners.html)
which uses statistical significance as
an heuristic for decision-making. It prunes repeated trainings like in a cross validation.
As the test method a t-test is used.

Optuna's standard pruners assume that you only adjust the model once per
hyperparameter set. Those pruners work on the basis of intermediate results. For example, once per
epoch. In contrast, this pruner does not work on intermediate results but on the results of a
cross validation or more precisely the results of the individual folds.

Below is a minimalist example:

```python
from hpoflow import SignificanceRepeatedTrainingPruner
import logging
import numpy as np
import optuna
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# configure the logger to see the debug output from the pruner
logging.getLogger().addHandler(logging.StreamHandler())
logging.getLogger("hpoflow.optuna").setLevel(logging.DEBUG)

dataset = load_iris()

x, y = dataset['data'], dataset['target']

def train(trial):
    parameter = {
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'n_estimators': trial.suggest_int('n_estimators', 20, 100),
    }

    validation_result_list = []

    skf = StratifiedKFold(n_splits=10)
    for fold_index, (train_index, val_index) in enumerate(skf.split(x, y)):
        X_train, X_val = x[train_index], x[val_index]
        y_train, y_val = y[train_index], y[val_index]

        rf = RandomForestClassifier(**parameter)
        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_val)

        acc = accuracy_score(y_val, y_pred)
        validation_result_list.append(acc)

        # report result of this fold
        trial.report(acc, fold_index)

        # check if we should prune
        if trial.should_prune():
            # prune here - we are done with this CV
            break

    return np.mean(validation_result_list)

study = optuna.create_study(
    storage="sqlite:///optuna.db",
    study_name="iris_cv",
    direction="maximize",
    load_if_exists=True,
    sampler=optuna.samplers.TPESampler(multivariate=True),
    # add pruner to optuna
    pruner=SignificanceRepeatedTrainingPruner(
        alpha=0.4,
        n_warmup_steps=4,
    )
)

study.optimize(train, n_trials=10)
```

## Licensing

Copyright (c) 2023 Philip May\
Copyright (c) 2023 Philip May, Deutsche Telekom AG

Licensed under the **MIT License** (the "License"); you may not use this file except in compliance with the License.
You may obtain a copy of the License by reviewing the file
[LICENSE](https://github.com/telekom/mltb2/blob/main/LICENSE) in the repository.
